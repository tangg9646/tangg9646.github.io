<!doctype html>
<html lang="en-us">
  <head>
    <title>机器学习的经典算法总结 // 唐广的个人博客</title>
    <meta charset="utf-8" />
    <meta name="generator" content="Hugo 0.59.1" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="author" content="TG" />
    <meta name="description" content="" />
    <link rel="stylesheet" href="https://tangg9646.github.io/css/main.min.f90f5edd436ec7b74ad05479a05705770306911f721193e7845948fb07fe1335.css" />

    
    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="机器学习的经典算法总结"/>
<meta name="twitter:description" content="本文整理了入门python机器学习最基本的算法，可作为手册使用
梳理了python代码，方便快速从这个手册中构建出相应的代码应用于自己的项目中
1.KNN 分类算法 由于knn算法涉及到距离的概念，KNN 算法需要先进行归一化处理
1.1 归一化处理 scaler from sklearn.preprocessing import StandardScaler standardScaler =StandardScaler() standardScaler.fit(X_train) X_train_standard = standardScaler.transform(X_train) X_test_standard = standardScaler.transform(X_test)  归一化之后送入模型进行训练
from sklearn.neighbors import KNeighborsClassifier knn_clf = KNeighborsClassifier(n_neighbors=8) knn_classifier.fit(X_train_standard, y_train) y_predict = knn_clf.predict(X_test_standard) # 默认的预测指标为分类准确度 knn_clf.score(X_test, y_test)  1.2 网格搜索 GridSearchCV 使用网格搜索来确定KNN算法合适的超参数
from sklearn.model_selection import GridSearchCV param_grid = [ { &#39;weights&#39;:[&#39;uniform&#39;], &#39;n_neighbors&#39;:[ i for i in range(1, 11)] }, { &#39;weights&#39;:[&#39;distance&#39;], &#39;n_neighbors&#39;:[i for i in range(1, 11)], &#39;p&#39;:[p for p in range(1, 6)] } ] grid_search = GridSearchCV(knn_clf, param_grid, n_jobs=-1, verbose=2) grid_search."/>

    <meta property="og:title" content="机器学习的经典算法总结" />
<meta property="og:description" content="本文整理了入门python机器学习最基本的算法，可作为手册使用
梳理了python代码，方便快速从这个手册中构建出相应的代码应用于自己的项目中
1.KNN 分类算法 由于knn算法涉及到距离的概念，KNN 算法需要先进行归一化处理
1.1 归一化处理 scaler from sklearn.preprocessing import StandardScaler standardScaler =StandardScaler() standardScaler.fit(X_train) X_train_standard = standardScaler.transform(X_train) X_test_standard = standardScaler.transform(X_test)  归一化之后送入模型进行训练
from sklearn.neighbors import KNeighborsClassifier knn_clf = KNeighborsClassifier(n_neighbors=8) knn_classifier.fit(X_train_standard, y_train) y_predict = knn_clf.predict(X_test_standard) # 默认的预测指标为分类准确度 knn_clf.score(X_test, y_test)  1.2 网格搜索 GridSearchCV 使用网格搜索来确定KNN算法合适的超参数
from sklearn.model_selection import GridSearchCV param_grid = [ { &#39;weights&#39;:[&#39;uniform&#39;], &#39;n_neighbors&#39;:[ i for i in range(1, 11)] }, { &#39;weights&#39;:[&#39;distance&#39;], &#39;n_neighbors&#39;:[i for i in range(1, 11)], &#39;p&#39;:[p for p in range(1, 6)] } ] grid_search = GridSearchCV(knn_clf, param_grid, n_jobs=-1, verbose=2) grid_search." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tangg9646.github.io/post/blog/" />
<meta property="article:published_time" content="2019-11-27T14:56:31+08:00" />
<meta property="article:modified_time" content="2019-11-27T14:56:31+08:00" />


  </head>
  <body>
    <header class="app-header">
      <a href="https://tangg9646.github.io/"><img class="app-header-avatar" src="https://upload-images.jianshu.io/upload_images/19168686-5c06ac3debe107b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="TG" /></a>
      <h1>唐广的个人博客</h1>
      <p>尝试用hugo平台构建我的个人博客，用于记录学习笔记以及自己的想法</p>
      <div class="app-header-social">
        
          <a target="_blank" href="https://github.com/tangg9646" rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-link">
  <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
  <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
</svg></a>
        
      </div>
    </header>
    <main class="app-container">
      
  <article class="post">
    <header class="post-header">
      <h1 class ="post-title">机器学习的经典算法总结</h1>
      <div class="post-meta">
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar">
  <title>calendar</title>
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line>
</svg>
          Nov 27, 2019
        </div>
        <div>
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock">
  <title>clock</title>
  <circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline>
</svg>
          8 min read
        </div></div>
    </header>
    <div class="post-content">
      

<p>本文整理了入门python机器学习最基本的算法，可作为手册使用</p>

<p>梳理了python代码，方便快速从这个手册中构建出相应的代码应用于自己的项目中</p>

<h1 id="1-knn-分类算法">1.KNN 分类算法</h1>

<p>由于knn算法涉及到<strong>距离</strong>的概念，KNN 算法需要先进行<strong>归一化处理</strong></p>

<h2 id="1-1-归一化处理-scaler">1.1 归一化处理 scaler</h2>

<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

standardScaler =StandardScaler()

standardScaler.fit(X_train)
X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)
</code></pre>

<p>归一化之后送入模型进行训练</p>

<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier(n_neighbors=8)
knn_classifier.fit(X_train_standard, y_train)
y_predict = knn_clf.predict(X_test_standard)

# 默认的预测指标为分类准确度
knn_clf.score(X_test, y_test)
</code></pre>

<h2 id="1-2-网格搜索-gridsearchcv">1.2 网格搜索 GridSearchCV</h2>

<p>使用网格搜索来确定KNN算法合适的超参数</p>

<pre><code class="language-python">from sklearn.model_selection import GridSearchCV

param_grid = [
    {
        'weights':['uniform'],
        'n_neighbors':[ i for i in range(1, 11)]
    },
    {
        'weights':['distance'],
        'n_neighbors':[i for i in range(1, 11)],
        'p':[p for p in range(1, 6)]
    }
]

grid_search = GridSearchCV(knn_clf, param_grid, n_jobs=-1, verbose=2)
grid_search.fit(X_train_standard, y_train)
knn_clf = grid_search.best_estimator_
knn_clf.score(X_test_standard, y_test)
</code></pre>

<h2 id="1-3-交叉验证">1.3 交叉验证</h2>

<ul>
<li>GridSearchCV 本身就包括了交叉验证，也可自己指定参数cv</li>
</ul>

<p>默认GridSearchCV的KFold平分为3份</p>

<ul>
<li><p>自己指定交叉验证，查看交叉验证成绩</p>

<pre><code class="language-python">from sklearn.model_selection import cross_val_score
  
# 默认为分成3份
cross_val_score(knn_clf, X_train, y_train, cv=5)
</code></pre></li>
</ul>

<hr />

<h1 id="2-线性回归">2. 线性回归</h1>

<h2 id="2-1-简单线性回归">2.1 简单线性回归</h2>

<pre><code class="language-python">from sklearn.linear_model import LinearRegression

linreg = LinearRegression()

linreg.fit(X_train, y_train)
</code></pre>

<p>查看截距和系数</p>

<pre><code class="language-python">print linreg.intercept_
print linreg.coef_
lin_reg.score(X_test, y_test)

y_predict = linreg.predict(X_test)
</code></pre>

<h2 id="2-2-多元线性回归">2.2 多元线性回归</h2>

<p>在更高维度的空间中的“直线”，即数据不只有一个维度，而具有多个维度</p>

<p>代码和上面的简单线性回归相同</p>

<hr />

<h1 id="3-梯度下降法">3. 梯度下降法</h1>

<p>使用梯度下降法之前，需要对数据进行<strong>归一化处理</strong></p>

<h2 id="3-1-随机梯度下降线性回归">3.1 随机梯度下降线性回归</h2>

<p><strong>SGD_reg</strong></p>

<pre><code class="language-python">from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(max_iter=100)
sgd_reg.fit(X_train_standard, y_train_boston)
sgd_reg.score(X_test_standard, y_test_boston)
</code></pre>

<h2 id="3-2-确定梯度下降计算的准确性">3.2 确定梯度下降计算的准确性</h2>

<p>以多元线性回归的<strong>目标函数（损失函数）</strong>为例</p>

<p>比较 使用<strong>数学推导式</strong>（得出具体解析解）的方法和<strong>debug的近似方法</strong>的比较</p>

<pre><code class="language-python"># 编写损失函数
def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
    except:
        return float('inf')
        
# 编写梯度函数（使用数学推导方式得到的）
def dJ_math(theta, X_b, y):
    return X_b.T.dot(X_b.dot(theta) - y) * 2.0 / len(y)
    
# 编写梯度函数（用来debug的形式）
def dJ_debug(theta, X_b, y, epsilon=0.01):
    res = np.empty(len(theta))
    for i in range(len(theta)):
        theta_1 = theta.copy()
        theta_1[i] += epsilon
        theta_2 = theta.copy()
        theta_2[i] -= epsilon
        res[i] = (J(theta_1, X_b, y) - J(theta_2, X_b, y)) / (2 * epsilon)
    return res

# 批量梯度下降，寻找最优的theta
def gradient_descent(dJ, X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
    theta = initial_theta
    i_iter = 0
    
    while i_iter &lt; n_iters:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        
        if(abs(J(theta, X_b, y) - J(last_theta, X_b, y)) &lt; epsilon):
            break
        
        i_iter += 1
    return theta

# 函数入口参数第一个，要指定dJ函数是什么样的
</code></pre>

<pre><code class="language-python">X_b = np.hstack([np.ones((len(X), 1)), X])
initial_theta = np.zeros(X_b.shape[1])
eta = 0.01

# 使用debug方式
theta = gradient_descent(dJ_debug, X_b, y, initial_theta, eta)
# 使用数学推导方式
theta = gradient_descent(dJ_math, X_b, y, initial_theta, eta)
# 得出的这两个theta应该是相同的
</code></pre>

<hr />

<h1 id="4-pca算法">4. PCA算法</h1>

<p>由于是求方差最大，因此使用的是<strong>梯度上升法</strong></p>

<p>PCA算法<strong>不能</strong>在前处理进行<strong>归一化处理</strong>，否则将会找不到主成分</p>

<h2 id="4-1-代码流程">4.1 代码流程</h2>

<pre><code class="language-python"># 对于二维的数据样本来说
from sklearn.decomposition import PCA

pca = PCA(n_components=1) #指定需要保留的前n个主成分，不指定为默认保留所有
pca.fit(X)

# 查看前n个主成分
pca.components_
</code></pre>

<p>比如，要使用KNN分类算法，先进行数据的降维操作</p>

<pre><code class="language-python">from sklearn.decomposition import PCA

pca = PCA(n_components=2)  #这里也可以给一个百分比，代表想要保留的数据的方差占比
pca.fit(X_train)

#训练集和测试集需要进行相同降维处理操作
X_train_reduction = pca.transform(X_train)
X_test_reduction = pca.transform(X_test)

#降维完成后就可以送给模型进行拟合
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train_reduction, y_train)
knn_clf.score(X_test_reduction, y_test)
</code></pre>

<h2 id="4-2-降维的维数和精度的取舍">4.2 降维的维数和精度的取舍</h2>

<p>指定的维数，能解释原数据的方差的<strong>比例</strong></p>

<pre><code class="language-python">pca.explained_variance_ratio_

# 指定保留所有的主成分
pca = PCA(n_components=X_train.shape[1])
pca.fit(X_train)
pca.explained_variance_ratio_

# 查看降维后特征的维数
pca.n_components_
</code></pre>

<p>把数据降维到2维，可以进行scatter的可视化操作</p>

<h2 id="4-3-pca数据降噪">4.3 PCA数据降噪</h2>

<p>先使用pca降维，之后再反向，升维</p>

<pre><code class="language-python">from sklearn.decomposition import PCA

pca = PCA(0.7)
pca.fit(X)
pca.n_components_

X_reduction = pca.transform(X)
X_inversed = pca.inverse_transform(X_reduction)
</code></pre>

<hr />

<h1 id="5-多项式回归于模型泛化">5. 多项式回归于模型泛化</h1>

<p>多项式回顾需要指定最高的阶数， <strong>degree</strong></p>

<p>拟合的将不再是一条直线</p>

<ul>
<li>只有一个特征的样本，进行多项式回归可以拟合出曲线，并且在二维平面图上进行绘制</li>
<li>而对于具有多个特征的样本，同样可以进行多项式回归，但是不能可视化拟合出来的曲线</li>
</ul>

<h2 id="5-1-多项式回归和pipeline">5.1 多项式回归和Pipeline</h2>

<pre><code class="language-python">from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline


poly_reg = Pipeline([
    (&quot;poly&quot;, PolynomialFeatures(degree=2)),
    (&quot;std_scaler&quot;, StandardScaler()),
    (&quot;lin_reg&quot;, LinearRegression())
])

poly_reg.fit(X, y)
y_predict = poly_reg.predict(X)

# 对二维数据点可以绘制拟合后的图像
plt.scatter(X, y)
plt.plot(np.sort(x), y_predict[np.argsort(x)], color='r')
plt.show()
</code></pre>

<pre><code class="language-python">#更常用的是，把pipeline写在函数中
def PolynomialRegression(degree):
    return Pipeline([
        (&quot;poly&quot;, PolynomialFeatures(degree=degree)),
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;lin_reg&quot;, LinearRegression())
    ])

poly2_reg = PolynomialRegression(degree=2)
poly2_reg.fit(X, y)

y2_predict = poly2_reg.predict(X)
mean_squared_error(y, y2_predict)
</code></pre>

<h2 id="5-2-gridsearchcv-和-pipeline">5.2 GridSearchCV 和 Pipeline</h2>

<p>明确：</p>

<ul>
<li>GridSearchCV：用于寻找给定模型的最优的参数</li>
<li>Pipeline：用于将几个流程整合在一起（PolynomialFeatures()、StandardScaler()、LinearRegression()）</li>
</ul>

<p>如果非要把上两者写在一起，应该把指定好param_grid参数的grid_search作为成员，传递给Pipeline</p>

<h2 id="5-3-模型泛化之岭回归-ridge">5.3 模型泛化之岭回归（Ridge）</h2>

<p>首先明确：</p>

<ul>
<li>模型泛化是为了解决<strong>模型过拟合</strong>的问题</li>
<li>岭回归是<strong>模型正则化</strong>的一种处理方式，也称为<strong>L2正则化</strong></li>

<li><p>岭回归是<strong>线性回归</strong>的一种正则化处理后的模型（作为pipeline的成员使用）</p>

<pre><code class="language-python">from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def RidgeRegression(degree, alpha):
return Pipeline([
    (&quot;poly&quot;, PolynomialFeatures(degree=degree)),
    (&quot;std_scaler&quot;, StandardScaler()),
    (&quot;ridge_reg&quot;, Ridge(alpha=alpha))
])

ridge_reg = RidgeRegression(degree=20, alpha=0.0001)
ridge_reg.fit(X_train, y_train)

y_predict = ridge_reg.predict(X_test)
mean_squared_error(y_test, y_predict)
</code></pre></li>
</ul>

<hr />

<p>代码中：</p>

<p>alpha为L2正则项前面的系数，代表的含义与LASSO回归相同</p>

<ul>
<li>alpha越小，越倾向于选择<strong>复杂模型</strong></li>
<li>alpha越大，越倾向于选择<strong>简单模型</strong></li>
</ul>

<p>Ridge回归、LASSO回归的区别</p>

<ul>
<li>Ridge：更倾向于保持为<strong>曲线</strong></li>
<li>LASSO： 更倾向于变为<strong>直线</strong>（即趋向于使得部分theta变成0， 因此有<strong>特征选择</strong>的作用）</li>
</ul>

<hr />

<h2 id="5-4-模型泛化之lasso回归">5.4 模型泛化之LASSO回归</h2>

<ul>
<li>岭回归是<strong>模型正则化</strong>的一种处理方式，也称为<strong>L1正则化</strong></li>

<li><p>岭回归是<strong>线性回归</strong>的一种正则化处理后的模型（作为pipeline的成员使用）</p>

<pre><code class="language-python">from sklearn.linear_model import Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def LassoRegression(degree, alpha):
return Pipeline([
    (&quot;poly&quot;, PolynomialFeatures(degree=degree)),
    (&quot;std_scaler&quot;, StandardScaler()),
    (&quot;lasso_reg&quot;, Lasso(alpha=alpha))
])

lasso_reg = LassoRegression(3, 0.01)
lasso_reg.fit(X_train, y_train)

y_predict = lasso_reg.predict(X_test)
mean_squared_error(y_test, y_predict)
</code></pre></li>
</ul>

<hr />

<h1 id="6-逻辑回归">6. 逻辑回归</h1>

<p>将样本特征与样本发生的<strong>概率</strong>联系起来。</p>

<ul>
<li>既可看做回归算法，也可分类算法</li>
<li>通常作为二分类算法</li>
</ul>

<h2 id="6-1-绘制决策边界">6.1 绘制决策边界</h2>

<pre><code class="language-python"># 不规则决策边界绘制方法
def plot_decision_boundary(model, axis):

    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(-1, 1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A', '#FFF59D', '#90CAF9'])

    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
    
    
#此处为线性逻辑回归
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
log_reg.score(X_test, y_test)
</code></pre>

<p>绘制决策边界</p>

<pre><code class="language-python">plot_decision_boundary(log_reg, axis=[4, 7.5, 1.5, 4.5])
plt.scatter(X[y==0, 0], X[y==0, 1], color='r')
plt.scatter(X[y==1, 0], X[y==1, 1], color='blue')
plt.show()
</code></pre>

<h2 id="6-2-多项式逻辑回归">6.2 多项式逻辑回归</h2>

<p>同样，类似于多项式回归，需要使用<strong>Pipeline构造多项式特征项</strong></p>

<pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

def PolynomialLogisticRegression(degree):
    return Pipeline([
        ('poly',PolynomialFeatures(degree=degree)),
        ('std_scaler',StandardScaler()),
        ('log_reg',LogisticRegression())
    ])

poly_log_reg = PolynomialLogisticRegression(degree=2)
poly_log_reg.fit(X, y)
poly_log_reg.score(X, y)
</code></pre>

<p>如果有需要，可以绘制出决策边界</p>

<pre><code class="language-python">plot_decision_boundary(poly_log_reg, axis=[-4, 4, -4, 4])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()
</code></pre>

<h2 id="6-3-逻辑回归中的正则化项和惩罚系数c">6.3 逻辑回归中的正则化项和惩罚系数C</h2>

<p>公式为：</p>

<p><strong>C * J(θ) + L1</strong></p>

<p><strong>C * J(θ) + L2</strong></p>

<p>上式中：</p>

<ul>
<li>C越大，L1、L2的作用越弱，模型越倾向<strong>复杂</strong></li>

<li><p>C越小，相对L1、L2作用越强， J(θ) 作用越弱，模型越倾向<strong>简单</strong></p>

<pre><code class="language-python">def PolynomialLogisticRegression(degree, C, penalty='l2'):
return Pipeline([
    ('poly',PolynomialFeatures(degree=degree)),
    ('std_scaler',StandardScaler()),
    ('log_reg',LogisticRegression(C = C, penalty=penalty))
    # 逻辑回归模型，默认为 penalty='l2'
])
</code></pre></li>
</ul>

<h2 id="6-4-ovr-和-ovo">6.4 OVR 和 OVO</h2>

<p>将只适用于二分类的算法，改造为适用于多分类问题</p>

<p>scikit封装了<strong>OvO OvR</strong>这两个类，方便其他二分类算法，使用这两个类实现多分类</p>

<p>例子中：log_reg是已经创建好的逻辑回归二分类器</p>

<pre><code class="language-python">from sklearn.multiclass import OneVsRestClassifier

ovr = OneVsRestClassifier(log_reg)
ovr.fit(X_train, y_train)
ovr.score(X_test, y_test)


from sklearn.multiclass import OneVsOneClassifier

ovo = OneVsOneClassifier(log_reg)
ovo.fit(X_train, y_train)
ovo.score(X_test, y_test)
</code></pre>

<hr />

<h1 id="7-支撑向量机svm">7. 支撑向量机SVM</h1>

<p><strong>注意</strong></p>

<ul>
<li>由于涉及到<strong>距离</strong>的概念，因此，在SVM拟合之前，必须先进行<strong>数据标准化</strong>

<br /></li>
</ul>

<p>支撑向量机要满足的优化目标是：</p>

<p>使 “<strong>最优决策边界</strong>”  到与两个类别的最近的样本  的距离最远</p>

<p>即，使得 <strong>margin</strong> 最大化</p>

<p>分为：</p>

<ul>
<li>Hard Margin SVM</li>
<li>Soft Margin SVM</li>
</ul>

<h2 id="7-1-svm的正则化">7.1 SVM的正则化</h2>

<p>为了改善SVM模型的泛化能力，需要进行正则化处理，同样有L1、L2正则化</p>

<p>正则化即弱化限定条件，使得某些样本可以不再Margin区域内</p>

<p>惩罚系数 <strong>C</strong> 是乘在正则项前面的
$$
min\frac{1}{2}||w||^2+C\sum_{i=1}^{m}{\xi_i}\text{,L1正则项}
$$</p>

<p>$$
min\frac{1}{2}||w||^2+C\sum_{i=1}^{m}{\xi_i^2}  \text {,L2正则项}
$$</p>

<p><strong>变化规律</strong> ：</p>

<ul>
<li>C越大，容错空间越小，越偏向于Hard Margin</li>
<li>C越小，容错空间越大，越偏向于Soft Margin</li>
</ul>

<h2 id="7-2-线性svm">7.2 线性SVM</h2>

<pre><code class="language-python">from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(X)
X_standard = standardScaler.transform(X)

from sklearn.svm import LinearSVC
svc = LinearSVC(C=1e9)
svc.fit(X_standard, y)
</code></pre>

<p>简洁起见，可以用Pipeline包装起来</p>

<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

def Linear_svc(C=1.0):
    return Pipeline([
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;linearSVC&quot;, LinearSVC(C=C))
    ])
linear_svc = Linear_svc(C=1e5)
linear_svc.fit(X, y)
</code></pre>

<h2 id="7-3-多项式特征svm">7.3 多项式特征SVM</h2>

<p><strong>明确：使用多项式核函数的目的都是将数据升维，使得原本线性不可分的数据变得线性可分</strong></p>

<p>在SVM中使用多项式特征有两种方式</p>

<ul>
<li>使用线性SVM，通过pipeline将 **poly  、std 、 linear_svc ** 三个连接起来</li>
<li>使用<strong>多项式核函数SVM</strong>, 则Pipeline只用包装 <strong>std 、 kernelSVC</strong> 两个类</li>
</ul>

<h3 id="7-3-1-传统pipeline多项式svm">7.3.1 传统Pipeline多项式SVM</h3>

<pre><code class="language-python"># 传统上使用多项式特征的SVM
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

def PolynomialSVC(degree, C=1.0):
    return Pipeline([
        (&quot;ploy&quot;, PolynomialFeatures(degree=degree)),
        (&quot;std_standard&quot;, StandardScaler()),
        (&quot;linearSVC&quot;, LinearSVC(C=C))
    ])

poly_svc = PolynomialSVC(degree=3)
poly_svc.fit(X, y)
</code></pre>

<h3 id="7-3-2-多项式核函数svm">7.3.2 多项式核函数SVM</h3>

<pre><code class="language-python"># 使用多项式核函数的SVM

from sklearn.svm import SVC

def PolynomialKernelSVC(degree, C=1.0):
    return Pipeline([
        (&quot;std_standard&quot;, StandardScaler()),
        (&quot;kernelSVC&quot;, SVC(kernel='poly', degree=degree, C=C))
    ])

poly_kernel_svc = PolynomialKernelSVC(degree=3)
poly_kernel_svc.fit(X, y)
</code></pre>

<h3 id="7-3-3-高斯核svm-rbf">7.3.3 高斯核SVM（RBF）</h3>

<p>将原本是$m*n$的数据变为$m*m$</p>

<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline

def RBFkernelSVC(gamma=1.0):
    return Pipeline([
        (&quot;std_standard&quot;, StandardScaler()),
        (&quot;svc&quot;, SVC(kernel=&quot;rbf&quot;, gamma=gamma))
    ])

svc = RBFkernelSVC(gamma=1.0)
svc.fit(X, y)
</code></pre>

<p>超参数gamma  $\gamma$  规律：</p>

<ul>
<li>gamma越大，高斯核越“窄”，头部越“尖”</li>
<li>gamma越小，高斯核越“宽”，头部越“平缓”，图形叉得越开</li>
</ul>

<p>若gamma太大，会造成 <strong>过拟合</strong></p>

<p>若gamma太小，会造成 <strong>欠拟合</strong> ，决策边界变为 <strong>直线</strong></p>

<h2 id="7-4-使用svm解决回归问题">7.4 使用SVM解决回归问题</h2>

<p>指定margin区域垂直方向上的距离 $\epsilon$    epsilon</p>

<p>通用可以分为<strong>线性SVR</strong>和<strong>多项式SVR</strong></p>

<pre><code class="language-python">from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVR
from sklearn.svm import SVR
from sklearn.pipeline import Pipeline

def StandardLinearSVR(epsilon=0.1):
    return Pipeline([
        (&quot;std_scaler&quot;, StandardScaler()),
        (&quot;linearSVR&quot;, LinearSVR(epsilon=epsilon))
    ])

svr = StandardLinearSVR()
svr.fit(X_train, y_train)

svr.score(X_test, y_test)
# 可以使用cross_val_score来获得交叉验证的成绩，成绩更加准确
</code></pre>

<hr />

<h1 id="8-决策树">8. 决策树</h1>

<p>非参数学习算法、天然可解决多分类问题、可解决回归问题(取叶子结点的平均值)、非常容易产生过拟合</p>

<p>可以考虑使用网格搜索来寻找最优的超参数</p>

<p>划分的依据有 基于<strong>信息熵</strong> 、 基于<strong>基尼系数</strong> (scikit默认用gini，两者没有特别优劣之分)</p>

<p>ID3、C4.5都是使用“entropy&rdquo;评判方式</p>

<p>CART(Classification and Regression Tree)使用的是“gini&rdquo;评判方式</p>

<p>常用超参数：</p>

<ul>
<li>max_depth</li>
<li>min_samples_split （设置最小的可供继续划分的样本数量 ）</li>
<li>min_samples_leaf （指定叶子结点最小的包含样本的数量 ）</li>
<li>max_leaf_nodes （指定，最多能生长出来的叶子结点的数量 ）</li>
</ul>

<h2 id="8-1-分类">8.1 分类</h2>

<pre><code class="language-python">from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(max_depth=2, criterion=&quot;gini&quot;)
# dt_clf = DecisionTreeClassifier(max_depth=2, criterion=&quot;entropy&quot;)

dt_clf.fit(X, y)
</code></pre>

<h2 id="8-2-回归">8.2 回归</h2>

<pre><code class="language-python">from sklearn.tree import DecisionTreeRegressor

dt_reg = DecisionTreeRegressor()
dt_reg.fit(X_train, y_train)

dt_reg.score(X_test, y_test)
# 计算的是R2值
</code></pre>

<hr />

<h1 id="9-集成学习和随机森林">9. 集成学习和随机森林</h1>

<h2 id="9-1-hard-voting-classifier">9.1 Hard Voting Classifier</h2>

<p>把集中分类模型包装在一起，根据每种模型的投票结果来得出最终预测类别</p>

<p><strong>可以先使用网格搜索把每种模型的参数调至最优，再来Voting</strong></p>

<pre><code class="language-python">from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

voting_clf = VotingClassifier(estimators=[
    (&quot;log_clf&quot;,LogisticRegression()),
    (&quot;svm_clf&quot;, SVC()),
    (&quot;dt_clf&quot;, DecisionTreeClassifier())
], voting='hard')
voting_clf.fit(X_train, y_train)
voting_clf.score(X_test, y_test)
</code></pre>

<h2 id="9-2-soft-voting-classifier">9.2 Soft Voting Classifier</h2>

<p>更合理的投票应该考虑每种模型的权重，即考虑每种模型对自己分类结果的 <strong>有把握程度</strong></p>

<p>所以，每种模型都应该能估计<strong>结果的概率</strong></p>

<ul>
<li>逻辑回归</li>
<li>KNN</li>
<li>决策树（叶子结点一般不止含有一类数据，因此可以有概率）</li>

<li><p>SVM中的SVC（可指定probability参数为True）</p>

<pre><code class="language-python">soft_voting_clf = VotingClassifier(estimators=[
(&quot;log_clf&quot;,LogisticRegression()),
(&quot;svm_clf&quot;, SVC(probability=True)),
(&quot;dt_clf&quot;, DecisionTreeClassifier(random_state=666))
], voting='soft')

soft_voting_clf.fit(X_train, y_train)
soft_voting_clf.score(X_test, y_test)
</code></pre></li>
</ul>

<h2 id="9-3-bagging-放回取样">9.3 Bagging（放回取样）</h2>

<p>（1）Bagging(放回取样) 和 Pasting(不放回取样)，由参数 <strong>bootstrap</strong> 来指定</p>

<ul>
<li>True：放回取样</li>
<li>False：不放回取样</li>
</ul>

<p>（2）这类集成学习方法需要指定一个 <strong>base estimator</strong></p>

<p>（3）放回取样，会存在 <strong>oob  (out of bag)</strong> 的样本数据，比例约37%，正好作为测试集</p>

<blockquote>
<p>obb_score=True/False , 是否使用oob作为测试集</p>
</blockquote>

<p>（4）产生差异化的方式：</p>

<ul>
<li>只针对特征进行随机采样：random subspace</li>

<li><p>既针对样本，又针对特征随机采样： random patches</p>

<pre><code class="language-python">random_subspaces_clf = BaggingClassifier(DecisionTreeClassifier(),
                           n_estimators=500, max_samples=500,
                           bootstrap=True, oob_score=True,
                           n_jobs=-1,
                           max_features=1, bootstrap_features=True)
random_subspaces_clf.fit(X, y)
random_subspaces_clf.oob_score_
</code></pre>

<pre><code class="language-python">random_patches_clf = BaggingClassifier(DecisionTreeClassifier(),
                           n_estimators=500, max_samples=100,
                           bootstrap=True, oob_score=True,
                           n_jobs=-1,
                           max_features=1, bootstrap_features=True)
random_patches_clf.fit(X, y)
random_patches_clf.oob_score_
</code></pre></li>
</ul>

<p>参数解释：</p>

<blockquote>
<p>max_samples: 如果和样本总数一致，则不进行样本随机采样</p>

<p>max_features: 指定随机采样特征的个数（应小于样本维数）</p>

<p>bootstrap_features: 指定是否进行随机特征采样</p>

<p>oob_score: 指定是都用oob样本来评分</p>

<p>bootstrap: 指定是否进行放回取样</p>
</blockquote>

<h2 id="9-4-随机森林和extra-tree">9.4 随机森林和Extra-Tree</h2>

<h3 id="9-4-1-随机森林">9.4.1 随机森林</h3>

<p>随机森林是指定了 Base Estimator为<strong>Decision Tree</strong> 的Bagging集成学习模型</p>

<p>已经被scikit封装好，可以直接使用</p>

<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(n_estimators=500, random_state=666, oob_score=True, n_jobs=-1)
rf_clf.fit(X, y)
rf_clf.oob_score_

#因为随机森林是基于决策树的，因此，决策树的相关参数这里都可以指定修改
rf_clf2 = RandomForestClassifier(n_estimators=500, random_state=666, max_leaf_nodes=16, oob_score=True, n_jobs=-1)
rf_clf2.fit(X, y)
rf_clf.oob_score_
</code></pre>

<h3 id="9-4-2-extra-tree">9.4.2 Extra-Tree</h3>

<p>Base Estimator为<strong>Decision Tree</strong> 的Bagging集成学习模型</p>

<p>特点：</p>

<blockquote>
<p>决策树在结点划分上，使用随机的特征和阈值</p>

<p>提供了额外的随机性，可以抑制过拟合，但会增大Bias (偏差)</p>

<p>具有更快的训练速度</p>
</blockquote>

<pre><code class="language-python">from sklearn.ensemble import ExtraTreesRegressor
et_clf = ExtraTreesClassifier(n_estimators=500, bootstrap=True, \
                              oob_score=True, random_state=666)
et_clf.fit(X, y)
et_clf.oob_score_
</code></pre>

<h2 id="9-5-ada-boosting">9.5 Ada Boosting</h2>

<p>每个子模型模型都在尝试增强（boost）整体的效果，通过不断的模型迭代，更新样本点的<strong>权重</strong></p>

<p>Ada Boosting没有oob的样本，因此需要进行 <strong>train_test_split</strong></p>

<p>需要指定 <strong>Base Estimator</strong></p>

<pre><code class="language-python">from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=500)
ada_clf.fit(X_train, y_train)

ada_clf.score(X_test, y_test)
</code></pre>

<h2 id="9-6-gradient-boosting">9.6 Gradient Boosting</h2>

<p>训练一个模型m1， 产生错误e1</p>

<p>针对e1训练第二个模型m2， 产生错误e2</p>

<p>针对e2训练第二个模型m3， 产生错误e3</p>

<p>&hellip;&hellip;</p>

<p>最终的预测模型是：$m1+m2+m3+&hellip;$</p>

<p>Gradient Boosting是基于决策树的，不用指定Base Estimator</p>

<pre><code class="language-python">from sklearn.ensemble import GradientBoostingClassifier

gb_clf = GradientBoostingClassifier(max_depth=2, n_estimators=30)
gb_clf.fit(X_train, y_train)
gb_clf.score(X_test, y_test)
</code></pre>

<h2 id="总结">总结</h2>

<p>上述提到的集成学习模型，不仅可以用于解决分类问题，也可解决回归问题</p>

<pre><code class="language-python">from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
</code></pre>

<h3 id="例子">例子：</h3>

<p>决策树和Ada Boosting回归问题效果对比</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor

# 构造测试函数
rng = np.random.RandomState(1)
X = np.linspace(-5, 5, 200)[:, np.newaxis]
y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])

# 回归决策树
dt_reg = DecisionTreeRegressor(max_depth=4)
# 集成模型下的回归决策树
ada_dt_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),
                               n_estimators=200, random_state=rng)

dt_reg.fit(X, y)
ada_dt_reg.fit(X, y)

# 预测
y_1 = dt_reg.predict(X)
y_2 = ada_dt_reg.predict(X)

# 画图
plt.figure()
plt.scatter(X, y, c=&quot;k&quot;, label=&quot;trainning samples&quot;)
plt.plot(X, y_1, c=&quot;g&quot;, label=&quot;n_estimators=1&quot;, linewidth=2)
plt.plot(X, y_2, c=&quot;r&quot;, label=&quot;n_estimators=200&quot;, linewidth=2)
plt.xlabel(&quot;data&quot;)
plt.ylabel(&quot;target&quot;)
plt.title(&quot;Boosted Decision Tree Regression&quot;)
plt.legend()
plt.show()
</code></pre>

<p><img src="https://upload-images.jianshu.io/upload_images/19168686-c59db31b4f642981.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png" /></p>

<hr />

<h1 id="评价指标">评价指标</h1>

<h2 id="一-分类算法">一、分类算法</h2>

<p><strong>常用指标选择方式</strong></p>

<p>平衡分类问题：</p>

<blockquote>
<p>分类准确度、ROC曲线</p>
</blockquote>

<p>类别不平衡问题：</p>

<blockquote>
<p>精准率、召回率</p>
</blockquote>

<h3 id="1-分类准确度">1.分类准确度</h3>

<p>一般用于<strong>平衡分类问题（每个类比的可能性相同）</strong></p>

<pre><code class="language-python">from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_predict)  #(真值，预测值)
</code></pre>

<h3 id="2-混淆矩阵-精准率-召回率">2. 混淆矩阵、精准率、召回率</h3>

<ul>
<li>精准率：<strong><em>正确预测为1</em></strong>  的数量，占，<strong><em>所有预测为1</em></strong>的比例</li>
</ul>

<p>$$percision=\frac{TP}{TP+FP}$$</p>

<ul>
<li>召回率：<strong><em>正确预测为1</em></strong>  的数量，占，  <strong><em>所有确实为1</em></strong>的比例</li>
</ul>

<p>$$recall=\frac{TP}{TP+FN}$$</p>

<p><img src="https://upload-images.jianshu.io/upload_images/19168686-06cd0cfd9fa075f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="混淆矩阵.png" /></p>

<pre><code class="language-python"># 先真实值，后预测值
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_log_predict)

from sklearn.metrics import precision_score
precision_score(y_test, y_log_predict)

from sklearn.metrics import recall_score
recall_score(y_test, y_log_predict)
</code></pre>

<hr />

<p>多分类问题中的混淆矩阵</p>

<hr />

<ul>
<li><p>多分类结果的<strong>精准率</strong></p>

<pre><code class="language-python">from sklearn.metrics import precision_score
precision_score(y_test, y_predict, average=&quot;micro&quot;)
</code></pre></li>

<li><p>多分类问题中的<strong>混淆矩阵</strong></p>

<pre><code class="language-python">from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_predict)
</code></pre></li>

<li><p>移除对角线上分类正确的结果，可视化查看其它分类错误的情况</p></li>
</ul>

<p>同样，横坐标为<strong>预测值</strong>，纵坐标为<strong>真实值</strong></p>

<pre><code class="language-python">cfm = confusion_matrix(y_test, y_predict)
row_sums = np.sum(cfm, axis=1)
err_matrix = cfm / row_sums
np.fill_diagonal(err_matrix, 0)

plt.matshow(err_matrix, cmap=plt.cm.gray)
plt.show()
</code></pre>

<h3 id="3-f1-score">3.F1-score</h3>

<p>F1-score是<strong>精准率precision</strong>和<strong>召回率recall</strong>的调和平均数</p>

<p>$$F1 = \frac{2*precision*recall}{precision+recall}$$</p>

<pre><code class="language-python">from sklearn.metrics import f1_score

f1_score(y_test, y_predict)
</code></pre>

<h3 id="4-精准率和召回率的平衡">4.精准率和召回率的平衡</h3>

<p>可以通过调整阈值，改变精确率和召回率（默认阈值为0）</p>

<ul>
<li>拉高阈值，会提高精准率，降低召回率</li>

<li><p>降低阈值，会降低精准率，提高召回率</p>

<pre><code class="language-python"># 返回模型算法预测得到的成绩
# 这里是以  逻辑回归算法  为例
decision_score = log_reg.decision_function(X_test)

# 调整阈值为5
y_predict_2 = np.array(decision_score &gt;= 5, dtype='int')
# 返回的结果是0 、1
</code></pre></li>
</ul>

<h3 id="5-精准率-召回率曲线-pr曲线">5.精准率-召回率曲线（PR曲线）</h3>

<pre><code class="language-python">from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_test, decision_score)
# 这里的decision_score是上面由模型对X_test预测得到的对象
</code></pre>

<ul>
<li><p>绘制PR曲线</p>

<pre><code class="language-python"># 精确率召回率曲线
plt.plot(precisions, recalls)
plt.show()
</code></pre></li>

<li><p>将精准率和召回率曲线，绘制在同一张图中</p></li>
</ul>

<blockquote>
<p>注意，当取“最大的” threshold值的时候，精准率=1，召回率=0，</p>

<p>但是，这个最大的threshold没有对应的值</p>

<p>因此thresholds会少一个</p>
</blockquote>

<pre><code class="language-python">plt.plot(thresholds, precisions[:-1], color='r')
plt.plot(thresholds, recalls[:-1], color='b')
plt.show()
</code></pre>

<h3 id="6-roc曲线">6.ROC曲线</h3>

<p>Reciver Operation Characteristic Curve</p>

<ul>
<li>TPR： True Positive rate</li>
</ul>

<p><img src="https://upload-images.jianshu.io/upload_images/19168686-44219afbd79abcb1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png" /></p>

<ul>
<li>FPR： False Positive Rate</li>
</ul>

<p>$$ FPR=\frac{FP}{TN+FP}$$</p>

<p><img src="https://upload-images.jianshu.io/upload_images/19168686-bdc5e2c5cab4c241.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png" /></p>

<p><img src="https://upload-images.jianshu.io/upload_images/19168686-06cd0cfd9fa075f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="混淆矩阵.png" /></p>

<p>绘制ROC曲线</p>

<pre><code class="language-python">from sklearn.metrics import roc_curve

fprs, tprs, thresholds = roc_curve(y_test, decision_scores)

plt.plot(fprs, tprs)
plt.show()
</code></pre>

<p>计算ROC曲线下方的面积的函数</p>

<p>roc_ <strong>a</strong>rea_ <strong>u</strong>nder_ <strong>c</strong>urve_score</p>

<pre><code class="language-python">from sklearn.metrics import roc_auc_score

roc_auc_score(y_test, decision_scores)
</code></pre>

<p>曲线下方的面积可用于比较两个模型的好坏</p>

<h2 id="二-回归算法">二、回归算法</h2>

<h3 id="1-均方误差-mse">1.均方误差 MSE</h3>

<pre><code class="language-python">from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, y_predict)
</code></pre>

<h3 id="2-平均绝对值误差-mae">2.平均绝对值误差 MAE</h3>

<pre><code class="language-python">from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test, y_predict)
</code></pre>

<h3 id="3-均方根误差-rmse">3.均方根误差 RMSE</h3>

<p>scikit中没有单独定于均方根误差，需要自己对均方误差MSE开平方根</p>

<h3 id="4-r2评分">4.R2评分</h3>

<pre><code class="language-python">from sklearn.metrics import r2_score
r2_score(y_test, y_predict)
</code></pre>

<h3 id="5-学习曲线">5.学习曲线</h3>

<p>观察模型在<strong>训练数据集</strong>和<strong>测试数据集</strong>上的评分，随着<strong>训练数据集样本数增加</strong>的变化趋势。</p>

<pre><code class="language-python">import numpy as np
import matplot.pyplot as plt
from sklearn.metrics import mean_squared_error


def plot_learning_curve(algo, X_train, X_test, y_train, y_test):
    
    train_score = []
    test_score = []
    for i in range(1, len(X_train)+1):
        algo.fit(X_train[:i], y_train[:i])

        y_train_predict = algo.predict(X_train[:i])
        train_score.append(mean_squared_error(y_train[:i], y_train_predict))

        y_test_predict = algo.predict(X_test)
        test_score.append(mean_squared_error(y_test, y_test_predict))
    
    plt.plot([i for i in range(1, len(X_train)+1)], np.sqrt(train_score), label=&quot;train&quot;)
    plt.plot([i for i in range(1, len(X_train)+1)], np.sqrt(test_score), label=&quot;test&quot;)
    plt.legend()
    plt.axis([0,len(X_train)+1, 0, 4])

    plt.show()
 
# 调用
plot_learning_curve(LinearRegression(), X_train, X_test, y_train, y_test )
</code></pre>

    </div>
    <div class="post-footer">
      
    </div>
  </article>

    </main>
  </body>
</html>
